{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-bfd1e808726a>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/dan/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/dan/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /home/dan/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting temp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /home/dan/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting temp/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/dan/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting temp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting temp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/dan/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "3333 <class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'> 333\n"
     ]
    }
   ],
   "source": [
    "#esto al parecer descarga la data mnist, la variable es de tipo:\n",
    "#\"<class 'tensorflow.contrib.learn.python.learn.datasets.base.Datasets'>\"\n",
    "mnist = input_data.read_data_sets(\"temp/data/\", one_hot=True)\n",
    "\n",
    "print(3333, type(mnist), 333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the neurons-------------\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "n_classes = 10\n",
    "#----------------------\n",
    "batch_size = 100\n",
    "#----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, 784]) #features data matrix\n",
    "y = tf.placeholder(\"float\") # labels data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "    \n",
    "    hidden_1_layer = {\"weights\": tf.Variable(tf.random_normal([784, n_nodes_hl1])),\n",
    "                      \"biases\": tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    \n",
    "    hidden_2_layer = {\"weights\": tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                      \"biases\": tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    \n",
    "    hidden_3_layer = {\"weights\": tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                      \"biases\": tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    \n",
    "    output_layer = {\"weights\": tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                      \"biases\": tf.Variable(tf.random_normal([n_classes]))}\n",
    "    #dd = hidden_1_layer[\"biases\"]\n",
    "    \n",
    "    l1 = tf.add(tf.matmul(data, hidden_1_layer['weights']), hidden_1_layer['biases'])\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        #print(sess.run(l1))\n",
    "        #print(t)\n",
    "    l1 = tf.nn.relu(l1)\n",
    "    \n",
    "    l2 = tf.add(tf.matmul(l1, hidden_2_layer['weights']), hidden_2_layer['biases'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "    \n",
    "    l3 = tf.add(tf.matmul(l2, hidden_3_layer['weights']), hidden_3_layer['biases'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "    \n",
    "    output = tf.matmul(l3, output_layer['weights']) + output_layer['biases']\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = neural_network_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "    prediction = neural_network_model(x)\n",
    "    #cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction,y))\n",
    "    #cost function\n",
    "    cost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y) )\n",
    "    # learning rate 0.001\n",
    "    # optimizador, se usa adam en vez del descenso del gradiente como tal\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "    \n",
    "    \n",
    "    hm_epochs = 18\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        #sess.run(tf.initialize_all_variables())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #training----------------------------\n",
    "        for epoch in range(hm_epochs):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict = {x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "            print(\"Epoch\", epoch, \"complete out of\", hm_epochs, \"loss\", epoch_loss)\n",
    "        #-------------------------------------\n",
    "        \n",
    "        #comparamos la prediccion con los labels\n",
    "        correct = tf.equal(tf.argmax(prediction, 1),tf.argmax(y, 1))\n",
    "        \n",
    "        print(\"argmax\", tf.argmax(prediction, 1))\n",
    "        print(\"correct: \", correct)\n",
    "        \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print(\"accuracy:\",accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 complete out of 18 loss 1734024.8279266357\n",
      "Epoch 1 complete out of 18 loss 400288.13667297363\n",
      "Epoch 2 complete out of 18 loss 213430.3880429268\n",
      "Epoch 3 complete out of 18 loss 127959.90543848037\n",
      "Epoch 4 complete out of 18 loss 77774.29207909107\n",
      "Epoch 5 complete out of 18 loss 49372.388469696045\n",
      "Epoch 6 complete out of 18 loss 30520.944427780807\n",
      "Epoch 7 complete out of 18 loss 24070.659922270068\n",
      "Epoch 8 complete out of 18 loss 20009.148900844157\n",
      "Epoch 9 complete out of 18 loss 19054.801452869382\n",
      "Epoch 10 complete out of 18 loss 15676.060086633079\n",
      "Epoch 11 complete out of 18 loss 15743.378666147706\n",
      "Epoch 12 complete out of 18 loss 11544.228469625115\n",
      "Epoch 13 complete out of 18 loss 14031.833498942087\n",
      "Epoch 14 complete out of 18 loss 13690.428839716114\n",
      "Epoch 15 complete out of 18 loss 12289.849645078182\n",
      "Epoch 16 complete out of 18 loss 10005.030435555062\n",
      "Epoch 17 complete out of 18 loss 9452.956596255302\n",
      "argmax Tensor(\"ArgMax_5:0\", shape=(?,), dtype=int64)\n",
      "correct:  Tensor(\"Equal_1:0\", dtype=bool)\n",
      "accuracy: 0.9596\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
